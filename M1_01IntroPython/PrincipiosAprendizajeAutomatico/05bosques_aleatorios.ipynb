{"cells":[{"cell_type":"markdown","id":"0a92990a","metadata":{"papermill":{"duration":0.003734,"end_time":"2023-04-21T13:39:39.404228","exception":false,"start_time":"2023-04-21T13:39:39.400494","status":"completed"},"tags":[],"id":"0a92990a"},"source":["<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fbigdatamagazine.es%2Fwp-content%2Fuploads%2F2023%2F02%2FFOTO-OK-BDM-DIG-DATA-MAGAZINE.jpg&f=1&nofb=1&ipt=4061921fa0da07483f83edb036d31f25545b2cae889c7eeefebd576f6e0fe5f4\" style=\"width:300px; float: right; margin: 0 40px 40px 40px;\"></img>\n","\n","# Introducción\n","\n","Los árboles de decisión te dejan ante una decisión difícil. Un árbol profundo con muchas hojas tenderá al **sobreajuste**, ya que cada predicción se basa únicamente en los pocos datos históricos disponibles en su hoja correspondiente. Pero un árbol poco profundo con pocas hojas tendrá un bajo rendimiento, ya que no logra capturar suficientes distinciones en los datos originales.\n","\n","Incluso las técnicas de modelado más sofisticadas de la actualidad enfrentan esta tensión entre **subajuste** y **sobreajuste**. Sin embargo, muchos modelos incorporan ideas inteligentes que pueden mejorar el rendimiento. Vamos a estudiar el **bosque aleatorio** (*random forest*) como ejemplo.\n","\n","El bosque aleatorio utiliza múltiples árboles, y realiza una predicción promediando las predicciones de cada árbol individual. En general, ofrece una precisión predictiva mucho mejor que un único árbol de decisión y funciona bien con los parámetros predeterminados. Si continúas explorando modelos, encontrarás técnicas con rendimientos aún mejores, aunque muchas de ellas son sensibles a una correcta configuración de hiperparámetros.\n","\n","# Ejemplo\n","\n","Ya has visto el código para cargar los datos en varias ocasiones. Al finalizar la carga de datos, contamos con las siguientes variables:\n","- `train_X`\n","- `val_X`\n","- `train_y`\n","- `val_y`"]},{"cell_type":"code","execution_count":null,"id":"494a0a00","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"collapsed":true,"execution":{"iopub.execute_input":"2023-04-21T13:39:39.412247Z","iopub.status.busy":"2023-04-21T13:39:39.411336Z","iopub.status.idle":"2023-04-21T13:39:40.698389Z","shell.execute_reply":"2023-04-21T13:39:40.697107Z"},"papermill":{"duration":1.29443,"end_time":"2023-04-21T13:39:40.701401","exception":false,"start_time":"2023-04-21T13:39:39.406971","status":"completed"},"tags":[],"id":"494a0a00"},"outputs":[],"source":["import pandas as pd\n","\n","# Cargar los datos\n","melbourne_file_path = '/content/melb_data.csv'\n","melbourne_data = pd.read_csv(melbourne_file_path)\n","\n","# Filtrar las filas con valores faltantes\n","melbourne_data = melbourne_data.dropna(axis=0)\n","\n","# Seleccionar la variable objetivo y las características\n","y = melbourne_data.Price\n","melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea',\n","                      'YearBuilt', 'Lattitude', 'Longtitude']\n","X = melbourne_data[melbourne_features]\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Dividir los datos en conjuntos de entrenamiento y validación, tanto para características como para la variable objetivo\n","# La división se basa en un generador de números aleatorios. Proporcionar un valor numérico al argumento\n","# random_state garantiza que obtendremos la misma división cada vez que ejecutemos este script.\n","train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n"]},{"cell_type":"markdown","id":"198283de","metadata":{"papermill":{"duration":0.002356,"end_time":"2023-04-21T13:39:40.707259","exception":false,"start_time":"2023-04-21T13:39:40.704903","status":"completed"},"tags":[],"id":"198283de"},"source":["Construimos un modelo de bosque aleatorio de forma similar a como construimos un árbol de decisión en scikit-learn —esta vez utilizando la clase `RandomForestRegressor` en lugar de `DecisionTreeRegressor`."]},{"cell_type":"code","execution_count":null,"id":"56e51553","metadata":{"execution":{"iopub.execute_input":"2023-04-21T13:39:40.714270Z","iopub.status.busy":"2023-04-21T13:39:40.713858Z","iopub.status.idle":"2023-04-21T13:39:43.082960Z","shell.execute_reply":"2023-04-21T13:39:43.081112Z"},"papermill":{"duration":2.376585,"end_time":"2023-04-21T13:39:43.086338","exception":false,"start_time":"2023-04-21T13:39:40.709753","status":"completed"},"tags":[],"id":"56e51553","outputId":"d30a99c3-6daf-4662-ef2d-0aac4b65c694"},"outputs":[{"name":"stdout","output_type":"stream","text":["191669.7536453626\n"]}],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","forest_model = RandomForestRegressor(random_state=1)\n","forest_model.fit(train_X, train_y)\n","melb_preds = forest_model.predict(val_X)\n","print(mean_absolute_error(val_y, melb_preds))"]},{"cell_type":"markdown","id":"139a5697","metadata":{"papermill":{"duration":0.002548,"end_time":"2023-04-21T13:39:43.091591","exception":false,"start_time":"2023-04-21T13:39:43.089043","status":"completed"},"tags":[],"id":"139a5697"},"source":["# Conclusión\n","\n","Probablemente haya margen para seguir mejorando, pero esta ya es una gran mejora respecto al mejor error obtenido con el árbol de decisión, que fue de 250,000. Existen hiperparámetros que te permiten ajustar el rendimiento del modelo de Bosque Aleatorio, de forma similar a como ajustamos la profundidad máxima de un árbol de decisión individual.\n","\n","Sin embargo, una de las mejores características de los modelos de Random Forest es que, en general, funcionan razonablemente bien incluso sin necesidad de ajustar estos parámetros.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":16.19966,"end_time":"2023-04-21T13:39:43.924262","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-04-21T13:39:27.724602","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}